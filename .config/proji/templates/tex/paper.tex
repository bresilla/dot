
\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption,array}
\usepackage[pdftex]{graphicx}

\graphicspath{{images/}}
\begin{document}
\title{Using Deep Neural Networks for autonomous UAV navigation within an apple orchard}
\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
    \and
    \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
    \and
    \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
    \and
    \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
    \and
    \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
    \and
    \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID}
}

\maketitle

\begin{abstract}
With the increase of population in the world, the demand for quality food is increasing too. One of the biggest base of raw food production comes from Agriculture. In recent years, due to the demand, and other environmental factors have heavily influenced the way agricultural production is done. Automation and robotics for fruit and vegetable production/monitoring has become the new standard. In this paper we discuss an autonomous Unmanned Areal Vehicle (UAV) that would be able to navigate through the rows in an orchard environment. The UAV is comprised of a flight controller (APM stack), a microcontroller for analog reading of different sensors, and an On-Board Computer (OBC). Pictures are taken through a camera and streamed through WiFi to a Ground Control Computer (GCC) running a convolutional neural network model. Based on prior trainings, the model outputs three directions: RIGHT, LEFT and STRAIGHT. A moving average of multiple frames per second is extracted and sent to a build-in PID controller on the UAV. After error correction from this feedback, controller sends the direction to the drone using MAVLink protocol's radio channel overrides, thus performing autonomous navigation.
\end{abstract}

\begin{IEEEkeywords}
Robotics, UAV, Precision Agriculture, Orchard Management, Deep Learning.
\end{IEEEkeywords}

\section{Introduction}

Agriculture as one of the oldest occupation in the world, has seen many changes during centuries. Before Industrial Revolution was estimated that more 80\% of population were working as farmers, while now is estimated that number to be less than 2\%. One of the dominant changes that characterises a growing economy is the proportionate decline in agriculture sector. This phenomenon is commonly attributes to two facts: food is not as demanding as other goods and services, or/and the rapid development of new farming technologies leads to expanding food supplies per hectare and per worker.

\begin{figure}[thpb]
      \centering
      \includegraphics[width=\linewidth]{agridecline2.png}
      \caption{Distribution of the labour force by sector, 1850-2000}
      \label{fig:robot1}
\end{figure}

There is a significant decrease every step of the industrial revolution. Industry 1.0 brought mechanisation, water management etc.; Industry 2.0 brought steam engines, electricity, protected environments etc.; Industry 3.0 brought computers and smart appliances, GPS tractors and so on; while Industry 4.0 is foreseen to be the most significant one, bringing AI, Automation and Robotics.

The challenge of how we'll feed the overgrowing world population in the future - should certainly be sustainable, cost-effective and most importantly environmentally friendly. In order to feed 9.5 billion people that Food and Agriculture Organisation (FAO) predicts to inhabit the planet by 2050 while climate change is making more difficult to grow crops - is going to be done by Smart Farming, a high-tech and AI driven agricultural management system. Agriculture is highly repetitive, and such, many tasks can be automated. Each agricultural activities on the farm takes a lot of resource, for example planting, maintaining, and harvesting crops need money, energy, labour and resources. What if we can use technology to replace some of the human activities and guarantee efficiency? Thatâ€™s where artificial intelligence is coming in. Agriculture is slowly is getting digitalised where data drives production. In this sense, AI in agriculture is emerging in three major categories, (i) agricultural robotics, (ii) soil and crop monitoring, and (iii) predictive analytics \cite{Giusti_2016}

For a farmer robot to be fully autonomous, it needs to navigate through very diverse and harsh environment without the human supervision. Then perform a set of actions at specific location like: picking fruits, evolute a site, spray pesticide, cut branches, plant a seed, image scan a whole plant or take specified measurement. Controlled environments like greenhouses are more manageable because of "controllable environment" and better engineered infrastructure,and as result, sensor measurements produce less noise. Whereas outdoor environment are much harsher and generally not controllable, thus making far more difficult for mobile robots. Most of outdoor robot are equipped with GPS for sensing the location, but due to accuracy, they are often companioned with other sensors like IMUs, 3DCameras, Rotary Encoders to create a sensor fusion for a much precise action taking process. Robots nowadays are wirelessly connected to a central operator to both receive updated instructions regarding the mission, and report status and data. However, making an autonomous farm robot requires better controllers, localisation, communication and action taking systems. The technology is similar to that of autonomous cars applied to agitech. Where it differs is that farming robots often need to manipulate their environment, picking vegetables or fruits, applying pesticides in a localised manner, or planting seeds. All these tasks require sensing, manipulation, and processing of their own.

In fruit production, as it is with all other fields of agriculture, crop monitoring is extremely important. Quantity and quality estimation ahead of time makes planning for logistics and distributions significantly more effective. In this paper is discussed an autonomous Unmanned Areal Vehicle (UAV) flying under tree canopy, between two rows and under the anti-hail nets. In order for the robot to successfully follow the row, it has firstly to know the orchard and where is the starting row, then has to determine the path between two rows while maintaining the altitude and avoid any collision with lateral branches from the trees and void any other obstacles. The approach taken in this paper is of that: it considers the whole navigation as a classification task. By analysing the front face camera images, by using a convolutional neural network to classify the video frames stream into direction with respected weight on a single shot.

\section{Background / Formulation}
Orchards nowadays are very complex with multiple components and management procedures moving during vegetative period of the plant. There are many management decisions that often change structure and visuals of the orchard, in addition to the nature's randomity and complexity, thus making it an ever changing organism. In this perspective, hard-coding algorithms for specific task where randomness is infinite, is not a effective approach. The path between two rows of trees, is maintained by farmers in different ways, differently during the season, the same goes with the canopy, where plants starts without leaves and then later growing them. The UAV itself has to fly under anti-hail nets, 1.5m above the ground, making the whole path as a long corridor. Using deep learning approach the model is able to accommodate for changes and progressively learn how to navigate even when new scenarios are being dealt with.

\subsection{Materials}
The UAV uses a RaspberryPi gen.3 as a On-Board Computer (OBC) which in connected to flight controller (PixHawk board with ArduPlot software stack) through serial link. The OBC due to performance limitations can not run deep-learning algorithms on its own, however its an excellent power efficient computer running full LINUX inside. It primary use is as intermediary layer between flight controller and other devices: Ground Control Computer (GCC), microcontroller (Arduino) boards with different sensors, camera (RasPi Cam gen.2), and other radio-communication devices. The communication between OBC and Flight controller is done through MAVLink protocol. MAVProxy is used to multiply the device infos and status to other devices connected to it through wireless hotspot. Since it runs full LINUX and Robotics Operating System (ROS), with the help of MAVROS package, the whole UAV is controlled as any other robot inside ROS. OBC's camera is a RasPi Cam gen.2 which makes the image processing and encoding in its own chips, thus making it very easy to directly stream in network.

The input images taken from front-facing camera, are sent to Ground Control Computer GCC (CUDA capable) though WiFi. The computer runs the picture stream through a trained model that has three outputs: right, left, and straight. The moving average of three outputs is sent as MAVLink RC\_CHANEL\_OVERRIDE through OBC to PixHawk.


\begin{figure}[!t]
  \begin{tabular}[b]{cc}
    \begin{tabular}[b]{c}
      \begin{subfigure}[b]{0.4\columnwidth}
          \includegraphics[width=\textwidth]{0.png} \caption{LEFT}
          \label{fig:A} \end{subfigure}\\
      \begin{subfigure}[b]{0.4\columnwidth}
          \includegraphics[width=\textwidth]{1.png} \caption{STRAIGHT}
          \label{fig:B} \end{subfigure} \end{tabular}
  & \begin{subfigure}[b]{0.4\columnwidth}
      \includegraphics[width=\textwidth]{2.png} \caption{RIGHT}
      \label{fig:C} \end{subfigure} \end{tabular} \label{fig:ABC}
      \caption{Images taken from UAV - winter (trees without leaves)} \end{figure}

\begin{figure}[!t]
  \begin{tabular}[b]{cc}
    \begin{tabular}[b]{c}
      \begin{subfigure}[b]{0.4\columnwidth}
          \includegraphics[width=\textwidth]{0a.jpg} \caption{LEFT}
          \label{fig:A} \end{subfigure}\\
      \begin{subfigure}[b]{0.4\columnwidth}
          \includegraphics[width=\textwidth]{1a.jpg} \caption{STRAIGHT}
          \label{fig:B} \end{subfigure} \end{tabular}
  & \begin{subfigure}[b]{0.4\columnwidth}
      \includegraphics[width=\textwidth]{2a.jpg} \caption{RIGHT}
      \label{fig:C} \end{subfigure} \end{tabular} \label{fig:ABC}
      \caption{Images taken manually - autumn (trees with leaves)} \end{figure}

\subsection{Data Acquisition}
Data collection is done through the camera of the OBC in the UAV moving inside the orchard, controlled manually with radio controller. When UAV starts up, it automatically starts some scripts on OBC (the process is managed with crontab):
\begin{enumerate}
    \item Connect to known WiFi if any exist, else create hotspot
    \item Use MAVproxy protocols to sent and receive flight plans and commands
    \item Automatically output camera stream to network
\end{enumerate}
In the GCC, the camera feed is piped from network with netcat to a python program. The program takes the camera input, divides in frames, labels it and saves in a proper dataset.

The UAV is flown very carefully inside the rows while streaming the camera feed to GCC. The flight is done many times, in different rows and directions. However there have been three modes/categories, and for each mode a thousand pictures/frames were taken and labelled accordingly:
\begin{enumerate}
    \item LEFT: The drone would fly closer to the left row, and/or yawed (facing) the left side. Data collected were labelled as LEFT, model would return LEFT and RIGHT CHANNEL OVERRIDE would be sent. Fig \ref{fig:A}.
    \item STRAIGHT: The drone would fly the best position as much as it can, in the middle of the row, facing straight and having both rows symmetrical to each other. Data collected were labelled as STRAIGHT, model would return STRAIGHT and FORWARD CHANNEL OVERRIDE would be sent. Fig \ref{fig:B}.
    \item RIGHT: The drone would fly closer to the right row, and/or yawed (facing) the right side. Data collected were labelled as RIGHT, model would return RIGHT and LEFT CHANNEL OVERRIDE would be sent. Fig \ref{fig:C}.
\end{enumerate}

Pictures were captured during daytime in early late winter of 2018. Daytime is important as the RasPi Cam is very sensitive to light quality and light exposure.

In addition to this dataset, another set of images is used. The later set is taken manually (using smartphone camera), but during autumn of 2017, while the tree had leaves and the chlorophyll was still green. The set has 100 images per mode/category. This number discussed in results in details, proved to be very small. However the same model is run through both sets separately, and then together too.
\subsection{The Model}
To better manage different datasets and models, Nvidia's Deep Learning GPU Training System (DIGITS) is used. DIGITS is not in itself a machine-learning framework, rather is a wrapper for most used frameworks available. It simplifies the commonly machine-learning tasks such as managing datasets including train/validation/test splitting, designing and training different neural networks (on CUDA capable GPUs), real-time monitoring of the training process and visualisation of the process.

GoogLeNet is used as a DNN classifier. Because of the use of Inception modules, GoogLeNet is more versatile and computationally less expensive. A simple 3x3 kernel with 256 input channels and 256 output, would have an amount of 9x256x256 calculations. Such a network where every output is connected to every input, is referred as dense connection. And while in most of CNNs activation layer for those connection is often either zero or not valuable, proving that not all those input channels are connected to output ones. Despite many techniques developed to cut off those unnecessary connections, the computation needed is huge. Inception module of the model we chose to work with approximates a sparse CNN with a normal dense construction, and since the effective number is low (because of zeros and unnecessary activations) the number of convolutional filters is kept small. In addition it uses convolutions of different sizes to capture details at different scales (5x5, 3x3) And it uses the so called bottleneck layer 1x1, for reduction of computation requirements.
GoogLeNet is a 27 layer deep CNN, with 22 convolution and inception layers and 5 pooling layers. However the overall number of the independent blocks is well over 100.

\begin{figure}[thpb]
    \centering
      \includegraphics[width=\linewidth]{googlenet.png}
      \caption{GoogLeNet model structure}
      \label{fig:robot1}
\end{figure}

\subsection{Training}
Nvidia's DIGITS dealt with splitting training, validation and testing set. We decided to keep 15\% of images for training and 5\% for testing. Each images was of size 256x256. Through image augmentation (manually with a script), all LEFT images could be mirrored and produce RIGHT image and vice versa, while STRAIGHT images when mirrored created another STRAIGHT image. The model was trained for 60 epochs in Amazon's AWS S3 instance with NVIDIA's Tesla K80 with 12GB Memory. An Adaptive Movement Solver (ADAM Optimiser) with 0.002 base learning rate was chosen. A sigmoid decay of gamma 0.08 learning rate with 60 steps is used. After training for few hours, the model is downloaded a sa pre-trained model, and transferred to the ground control computer (with CUDA capabilities) that runs the transferred model through Nvidia'S GTX 960M GPU.

\begin{figure}
      \centering
      \includegraphics[width=\linewidth]{training.png}
      \caption{Training on 60 epochs}
      \label{fig:robot1}
\end{figure}

\begin{figure}
      \centering
      \includegraphics[width=\linewidth]{learning.png}
      \caption{Learning rate}
      \label{fig:robot1}
\end{figure}

\subsection{Navigation policy}

As briefly mentioned above, the OBC, after capturing the data,it sends them through WiFi to the GCC to run through the model. Since the OBC is not a powerful enough computer, this step is necessary. Pictures received from the network are piped to the Coffe2 model through Netcat (a GNU/Linux utility for reading from and writing to network connections using TCP and UDP protocol). OpenCV captures the Netcat pipe and serves to the model as frames (a single image where the model performs the recognition).

The model that has three outputs (LEFT, RIGHT, STRAIGHT), and based on recognition/classification confidence to each frame the values from 0 (not confident) to 1 (very confident) per each class. The outputs are changed into RC\_CHANEL\_OVERRIDE MAVLink protocol (with values from 1000 to 2000 per channel). Those values go through a moving average script, that takes the values for several frames (in our case all frames per one second) and extract the moving average or running mean values for each channel. Running mean of each channel is sent through ZeroMQ (an highly efficient, high-performance and reliable asynchronous messaging library) to the PID (Proportional Integral Derivative) controller that we build in the OBC.

\subsection {PID and Sensor Fusion}
(IN PROGRESS, when we finish more work on localisation and PID and add some references, the paper will be ready for publication, hopefully sooner than later. We are integrating the channel overrides, and other sensors (GPS, UltraSonic, LIDARv3 and Flow camera - facing down) all together and with MAVROS and ROS to a built PID controller, for object avoidance, auto navigation and so on. But as for now, we tested without them, just with simple CHANEL-OVERRIDE. All this is very difficult inside a very small space - a 3x3m corridor).
\begin{figure}
\centering
\subcaptionbox{LEFT}{%
  \includegraphics[width=0.45\textwidth]{left-prediction.png}%
  }\par\medskip
\subcaptionbox{STRAIGHT}{%
  \includegraphics[width=0.45\textwidth]{straight-prediction.png}%
  }\par\medskip
\subcaptionbox{RIGHT}{%
  \includegraphics[width=0.45\textwidth]{right-prediction.png}%
  }
\caption{Predictions per class}
\label{TS}
\end{figure}


\section{Restrictions and Limitations}
It does not go without mentioning that one of the major restrictions in this research is the uncontrolled environment. Sine the navigation relies the most on camera input, the light and sun exposure, the season of the year, and the vegetative growth of the trees makes the model to vary a lot. In this context, taking more data on different weather conditions, different seasons and when tree leaves have different colouring and are in the tree itself, would make the model fit more scenarios and be more accurate.


\section{Results}
\subsection{Deep-Learning model}
We run the model through test dataset and reached accuracy of more than 85\%. In the first models, we trained the winter batch (set 1 of images) and autumn batch (set 2 of images) separately, and when tested images from second dataset in the fist model, the accuracy is very low, under 20\%. And the same vice versa. The third model, that runs on all images together, reaches an accuracy more than 76\%. The testing set compromised of well over 200 images that were manually separated from DIGITS data frame. In cases where the front path is not seen in the image, the model gives equal (or almost equal) confidence score to both RIGHT and LEFT, as when path is not in the frame, they are almost identical, later this would be handled from the PID on the OBC.

\begin{figure}
\centering
\subcaptionbox{LEFT}{%
  \includegraphics[width=0.45\textwidth]{left-activation.png}%
  }\par\medskip
\subcaptionbox{STRAIGHT}{%
  \includegraphics[width=0.45\textwidth]{straight-activation.png}%
  }\par\medskip
\subcaptionbox{RIGHT}{%
    \includegraphics[width=0.45\textwidth]{right-activation.png}%
  }
  \caption{Activations of last convolutional layer}
\label{TS}
\end{figure}

\subsection{Autonomous navigation}
OBC is connected through serial to flight controller and sends direct navigation messages as CHANNEL\_OVERRIDE. Its astonishingly amazing to see how the OBS and flight controller handle the commands. The drone's facing down LIDAR and Flow camera keep it very steady in the position (in LOITER MODE - APM Flight stack) while the drone moves forward direction.

\section{Conclusion / Future work}
Using deep learning and convolutional neural networks makes it very easy for UAV and other mobile robots to navigate paths and spaces not explicitly programmed. In orchard environments, where the top of the orchard is usually covered with hail nets, its impossible to asses the trees and production for above. Our work shows that using a UAV inside the rows where, in the future this drone would handle specific sensors for precision measurement of fruit quality, fruit size estimation, tracking maturity of the fruits, controlling for diseases...

Since the navigation direction are a moving average of many frames per second, this makes the model quiet robust to even small errors or misclassification occur. In our case, one can argue that inference time is more important than accuracy.

A better model, taking into consideration all the restrictions mentioned before, will contribute to a robustness of classifications (in turn batter navigations). In this research we relied strongly on transferring of the data from OBC to GCC for classification. By using a better on-board computer we would gain better results as we would not have to relay on this transfer. A NVIDIA's Jetson platform or Intel's Movidious Compute Stick would run the model locally (in the edge, no need to relay on GCC), and would make the platform more reliable and more responsive.


\bibliography{bibliography.bib}
\bibliographystyle{ieeetr}

\end{document}
